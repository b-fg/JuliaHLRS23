{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gethostname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: GPU topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/gpu_topology.png\" width=1300px>\n",
    "\n",
    "**Source:** [Sivalingam, Karthee. \"GPU Acceleration of a Theoretical Particle Physics Application.\" Master's Thesis, The University of Edinburgh (2010).](https://static.epcc.ed.ac.uk/dissertations/hpc-msc/2009-2010/Karthee%20Sivalingam.pdf)\n",
    "\n",
    "* **SM** = Streaming Multiprocessor\n",
    "* **SP** = Streaming Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA A100 SXM4\n",
    "\n",
    "<img src=\"./imgs/a100_front.png\" width=600px>\n",
    "<br>\n",
    "\n",
    "**Streaming Multiprocessor:**\n",
    "\n",
    "<img src=\"./imgs/a100_SM.png\" width=600px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "\n",
    "| Kind                       | Count            |\n",
    "|----------------------------|------------------|\n",
    "| **SMs**                    | 108              |\n",
    "| **CUDA cores** / FP32 ALUs | 6912 (64 per SM) |\n",
    "| **Tensor cores**           | 432 (4 per SM)   |\n",
    "\n",
    "* **ALU** = Arithmetic Logical Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick comparison: CPU vs GPU\n",
    "\n",
    "### AMD EPYC 7763 vs NVIDIA A100\n",
    "\n",
    "|               | number of cores    | maximum clock frequency [GHz] | FP32 peak performance [TFLOPS] |\n",
    "|:-------------:|:------------------:|:-----------------------------:|:------------------------------:|\n",
    "| AMD EPYC 7763 |   64               |  3.50                         |  5.0                           |\n",
    "| NVIDIA A100   | 6912               |  1.41                         | 19.5 (**155.9** for Tensor cores)  |\n",
    "\n",
    "The computing power of the leading [top500](https://top500.org/lists/top500/2023/06/) systems lies in GPUs.\n",
    "\n",
    "### Differences between CPU and GPU\n",
    "\n",
    "|                   | CPU                               | GPU                                 |\n",
    "|:-----------------:|:---------------------------------:|:-----------------------------------:|\n",
    "| designed for      | task parallelism (MIMD/MISD)      | **data parallelism (SIMD)**         |\n",
    "| optimized for     | latency and per-core performance  | computational throughput            |\n",
    "| cores             | complex                           | rather simple                       |\n",
    "| number of threads | O(100)                            | **O(10000) (millions can be scheduled)** |\n",
    "| thread pinning    | a must for good performance       | not required                        |\n",
    "\n",
    "### Memory-bound scientific computing\n",
    "\n",
    "The performance of most scientific codes **memory-bound** (memory access speed) rather than compute-bound (how fast computations can be done). In a certain time interval, GPUs (and CPUs) can perform more computations than read numbers from memory.\n",
    "\n",
    "**Peak performance over peak memory bandwidth** (for A100)\n",
    "\n",
    "$$\n",
    "\\dfrac{19.5 \\ [\\textrm{TFlop/s}]}{1.5 \\ [\\textrm{TB/s}]} \\cdot 4 \\ \\textrm{B} = 52\n",
    "$$\n",
    "\n",
    "An A100 (using only CUDA cores) can thus perfrom 52 FLOPS per each number read (4 byte, i.e. `Float32`) from memory.\n",
    "\n",
    "**Floating point operations are essentially \"free\"** in this regime!\n",
    "\n",
    "Crucially, the peak memory bandwidth of GPUs is much higher than for CPUs: **~1.5 TB/s** (A100) vs **~400 GB/s** (2x AMD EPYC 7763).\n",
    "\n",
    "(‚Üí exercise **saxpy_gpu** and **daxpy_cpu** from yesterday)\n",
    "\n",
    "### GPU acceleration\n",
    "\n",
    "<img src=\"./imgs/Julia-code-cpu-gpu.png\" width=900px>\n",
    "\n",
    "**host**: System CPU(s) + system memory (host memory) etc.\n",
    "\n",
    "**device**: the GPU with its own memory (device memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia + GPU (NVIDIA)\n",
    "\n",
    "Website: https://juliagpu.org/\n",
    "\n",
    "We'll focus on **NVIDIA GPUs** but there is [support for other GPUs](https://juliagpu.org/) (AMD, Intel, etc.) as well.\n",
    "\n",
    "The interface to NVIDIA GPU computing is the [CUDA language extension](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html). In Julia there is [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl).\n",
    "\n",
    "It leverages LLVM, specifically parts of the Julia compiler as well as [GPUCompiler.jl](https://github.com/JuliaGPU/GPUCompiler.jl), to compile **native GPU code**. (compare to `nvcc`)\n",
    "\n",
    "It provides:\n",
    "\n",
    "* **High-level abstraction `CuArray`**\n",
    "* **Tools for writing custom CUDA kernels**\n",
    "* **Wrappers to proprietary NVIDIA libraries (e.g. CUBLAS, CUFFT, CUSPARSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting CUDA\n",
    "\n",
    "By default, **it's automatic**. The CUDA toolkit is installed automatically when **using** CUDA.jl for the first time. The only requirement is a working NVIDIA driver.\n",
    "\n",
    "**Note:** You can readily add CUDA.jl to a Julia environment on a machine without GPUs, say, a login node. See [Precompiling CUDA.jl without CUDA](https://cuda.juliagpu.org/stable/installation/overview/#Precompiling-CUDA.jl-without-CUDA) for more information.\n",
    "\n",
    "#### System CUDA\n",
    "\n",
    "You can opt-out of the automatic system by setting a Julia preference, e.g.\n",
    "\n",
    "```julia\n",
    "CUDA.set_runtime_version!(v\"12.2\"; local_toolkit=true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.functional() # if this works, you're good to go üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device() # the currently selected GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array programming: `CuArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use a GPU is via **vectorized array operations** (e.g. broadcasting). Each of these operations will be backed by one or more GPU kernels, either natively written in Julia or from some application library. As long as your data is large enough you should be able to get nice speed-ups in many cases.\n",
    "\n",
    "You use the `CuArray` type, which serves a dual porpose:\n",
    "\n",
    "* a managed container for GPU memory\n",
    "* a way to dispatch to operations that execute on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `CuArray` is a **CPU object representing GPU memory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = CuArray{Float32}(undef, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.rand(3) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can readily move data to the GPU by converting to `CuArray`.\n",
    "\n",
    " <img src=\"./imgs/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = [1,2,3]\n",
    "x_gpu = CuArray(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or by using `copyto!` to move it into already allocated memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array computations on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CuArray <: AbstractArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should be able to do all kind of operations with it, that we'd also do with regular `Array`s. (**duck typing**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "A_gpu = CUDA.rand(N,N)\n",
    "B_gpu = CUDA.rand(N,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu * B_gpu # we need CUDA.@sync because GPU operations are typically asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime CUDA.@sync(A_cpu * B_cpu) setup=(A_cpu = rand(Float32, N,N); B_cpu = rand(Float32, N,N););\n",
    "@btime CUDA.@sync(A_gpu * B_gpu) setup=(A_gpu = CUDA.rand(N,N); B_gpu = CUDA.rand(N,N););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: `*` for `CuArray`s uses a cuBLAS kernel under the hood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More examples: Broadcasting, `map`, `reduce`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu .+ B_gpu # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@sync sqrt.(A_gpu.^2 + B_gpu.^2) # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@sync mapreduce(sin, +, A_gpu) # runs on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The power of simple GPU array programming can not be underestimated!** Entire codes (like deep learning frameworks etc.) could be ported to GPU without ever writing a single CUDA kernel manually.\n",
    "\n",
    "(Of course, it isn't always as easy or performance can be improved by writing custom kernels. (-> exericse **heat_diffusion**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Counter-example:\" Scalar indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@allowscalar A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gpu_not_actually!(C, A, B)\n",
    "    CUDA.@sync CUDA.@allowscalar for i in eachindex(A,B)\n",
    "        C[i] = A[i] * B[i] # multiplication will happen on CPU!\n",
    "    end\n",
    "end\n",
    "\n",
    "function gpu_broadcasting!(C, A, B)\n",
    "    CUDA.@sync C .= A .* B\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "N = 10\n",
    "@btime gpu_not_actually!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););\n",
    "@btime gpu_broadcasting!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Side note: CUDA executor for FLoops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "using FLoops, FoldsCUDA\n",
    "\n",
    "function gpu_floops!(C, A, B)\n",
    "    CUDA.@sync @floop CUDAEx() for i in eachindex(A,B,C)\n",
    "        C[i] = A[i] * B[i]\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few words on memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CuArray`s are managed by Julia's **garbage collector**. If they are unreachable, they will get cleaned up automatically during a GC run. However, keep in mind that the (CPU-focused) GC isn't good at sensing GPU memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof(x_gpu) |> Base.format_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = nothing; GC.gc(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on?\n",
    "\n",
    "By default CUDA.jl uses a **memory pool** to speed up future allocations. So it might appear as if the objects have not been free'd. (You can disable the pool with `JULIA_CUDA_MEMORY_POOL=none`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CUDA.unsafe_free!(x_gpu)` and `CUDA.reclaim()` to more aggressively suggest the freeing of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(x_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one must be careful with `CUDA.unsafe_free!` because one still has the handle `x_gpu` that now points to free'd memory. But it is fine and very useful in a pattern like this:\n",
    "\n",
    "```julia\n",
    "function myfunction(x::CuArray)\n",
    "    tmp_memory = similar(x)\n",
    "    expensive_operation!(x, tmp_memory)\n",
    "    CUDA.unsafe_free!(tmp_memory)\n",
    "    return x\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel programming: Writing CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CUDA kernel is a function that will be executed by all GPU *threads* in parallel.\n",
    "\n",
    "Based on the index of a thread we can make them operate on different pieces of given data (SPMD/SIMD programming model similar to MPI).\n",
    "\n",
    "(It might be helpful to think of the CUDA kernel as being the body of a loop (that you never write).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cuda_kernel!(x)\n",
    "    i = threadIdx().x # the thread index (\"loop index\")\n",
    "    x[i] += 1\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can launch the kernel on the GPU with the `@cuda` macro (non-blocking, asynchronous):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1024)\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imaging, kernel programming can of course become (much) more difficult, especially if you care about performance. A few reasons:\n",
    "\n",
    "* you need to respect **hardware limitations** of the GPU\n",
    "* **not all operations can readily be expressed as scalar kernels** (Example: reduction)\n",
    "* kernels execute on the GPU where the **Julia runtime isn't available**\n",
    "\n",
    "In particular due to the last point, kernel code has limitations\n",
    "  * no GC\n",
    "  * no `print` etc. (-> `@cuprint`)\n",
    "  * code must be fully type inferred (no dynamic dispatch allowed)\n",
    "  * no `try ... catch ... end`\n",
    "  * ...\n",
    "\n",
    "**You can't just write arbitrary Julia code in kernels.** Fortunately though, many things just work and can get you far (see e.g. exercises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Hardware limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025) # one more element than before\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA programming model\n",
    "\n",
    "<img src=\"./imgs/cuda_blocks_threads.png\" width=700px>\n",
    "\n",
    "(Note: in Julia indices start at 1)\n",
    "\n",
    "**Source:** [Sivalingam, Karthee. \"GPU Acceleration of a Theoretical Particle Physics Application.\" Master's Thesis, The University of Edinburgh (2010).](https://static.epcc.ed.ac.uk/dissertations/hpc-msc/2009-2010/Karthee%20Sivalingam.pdf)\n",
    "\n",
    "Conceptual mapping:\n",
    "\n",
    "* **Threads** ‚Üí CUDA cores\n",
    "* **Blocks** of threads ‚Üí SMs\n",
    "* **Grid** of blocks ‚Üí entire GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cuda_kernel_blocks!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x # global thread index\n",
    "    if i <= length(x) # make sure that we're inbounds (c.f. \"loop\" iteration range)\n",
    "        @inbounds x[i] += 1\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025)\n",
    "\n",
    "CUDA.@sync @cuda threads=1024 blocks=2 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does our custom CUDA kernel compare to broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_one_kernel(x)\n",
    "    CUDA.@sync @cuda threads=1024 blocks=1 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_broadcasting(x)\n",
    "    CUDA.@sync x .+ 1\n",
    "end\n",
    "\n",
    "@btime add_one_kernel(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_broadcasting(x) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying kernel launches: [Occupancy API](https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/)\n",
    "\n",
    "Hardcoding limits (1024 above) is rarely a good idea. A few reasons:\n",
    "\n",
    "* In reality, the actual maximal number of threads can depend on kernel details, like how many resources the kernel is using.\n",
    "* You might want to support different GPUs with different hardware limitations.\n",
    "\n",
    "**The occupancy API is an automatic tool that can be used to obtain good launch parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = @cuda launch=false cuda_kernel_blocks!(x) # don't launch the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CUDA.launch_configuration(kernel.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the number `blocks` indicates how many blocks we would need to fully occupy the GPU. For a given input `x`, we might need fewer or more blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = min(length(x), config.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = cld(length(x), threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the kernel with the dynamic launch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel(x; threads, blocks) # calling `kernel` like a regular function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `@code_*` for CPU there are `@device_code_*` macros. However, the GPU pendant for `@code_native` is `@device_code_ptx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_warntype @cuda threads=1024 blocks=1 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_llvm debuginfo=:none @cuda threads=1024 blocks=1 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_ptx @cuda threads=1024 blocks=1 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks + GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Julia task gets its own local CUDA execution environment. That makes it easy to use, e.g., one task per device, or to use tasks for independent operations that can be overlapped.\n",
    "\n",
    "**Note:** In the following we will use `Threads.@spawn`. Since multithreading support is a rather recent addition to CUDA.jl, one might use `@async` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping CPU and GPU operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function overlap_cpu_and_gpu()\n",
    "    @sync begin\n",
    "        @spawn begin\n",
    "            println(\"GPU task: begin\")\n",
    "            A = CUDA.rand(1024, 1024)\n",
    "            B = CUDA.rand(1024, 1024)\n",
    "            A * B\n",
    "            println(\"GPU task: wait\")\n",
    "            CUDA.synchronize()\n",
    "            println(\"GPU task: end\")\n",
    "        end\n",
    "        @spawn begin\n",
    "            println(\"CPU task: begin\")\n",
    "            for x in 1:10\n",
    "                A = rand(2048, 2048)\n",
    "                B = rand(2048, 2048)\n",
    "                A .* B\n",
    "            end\n",
    "            println(\"CPU task: end\")\n",
    "        end\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_cpu_and_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping GPU operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With modern GPUs becoming more and more powerful, it's getting harder to have every kernel use all of the device's hardware resources.\n",
    "\n",
    "Potential solution: overlap multiple (streams of) GPU computations such that the GPU can overlap operations whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function gpu_computation(A, B, C)\n",
    "    mul!(C, A, B)\n",
    "    sin.(C)\n",
    "    return C\n",
    "end\n",
    "\n",
    "function overlap_gpu()\n",
    "    A = CUDA.rand(1024, 1024)\n",
    "    B = CUDA.rand(1024, 1024)\n",
    "    C = CUDA.zeros(1024, 1024)\n",
    "\n",
    "    D = CUDA.rand(1024, 1024)\n",
    "    E = CUDA.rand(1024, 1024)\n",
    "    F = CUDA.zeros(1024, 1024)\n",
    "\n",
    "    @sync begin\n",
    "        println(\"Spawning gpu_computation on GPU\")\n",
    "        @spawn CUDA.@sync gpu_computation(A, B, C)\n",
    "        println(\"Spawning gpu_computation on GPU\")\n",
    "        @spawn CUDA.@sync gpu_computation(D, E, F)\n",
    "        println(\"Waiting...\")\n",
    "    end\n",
    "    println(\"Everything done.\")\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU (same machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function multi_gpu()\n",
    "    @sync begin\n",
    "        @spawn begin\n",
    "            device!(0) # first GPU\n",
    "            A = CUDA.rand(1024, 1024)\n",
    "            B = CUDA.rand(1024, 1024)\n",
    "            C = CUDA.zeros(1024, 1024)\n",
    "\n",
    "            println(\"GPU 1: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 1: done\")\n",
    "        end\n",
    "\n",
    "        @spawn begin\n",
    "            device!(1) # second GPU\n",
    "            A = CUDA.rand(1024, 1024)\n",
    "            B = CUDA.rand(1024, 1024)\n",
    "            C = CUDA.zeros(1024, 1024)\n",
    "\n",
    "            println(\"GPU 2: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 2: done\")\n",
    "        end\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call CUDA.memory_status() for all GPUs\n",
    "for dev in devices()\n",
    "    device!(dev)\n",
    "    println()\n",
    "    CUDA.memory_status()\n",
    "end\n",
    "device!(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking + Profiling (comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device!(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "\n",
    "@btime CUDA.@sync A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"allocations here\" means CPU allocations. For GPU allocations you can e.g. use `CUDA.@time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@time A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated profiler: `CUDA.@profile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile A .* B;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile trace=true A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External profiler: [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://cuda.juliagpu.org/stable/development/profiling/#External-profilers\n",
    "\n",
    "**Command**: `CUDA.@profile external=true`\n",
    "\n",
    "Use [**NVTX.jl**](https://github.com/JuliaGPU/NVTX.jl) to annotate (i.e. label and colorize) code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/nsight_systems.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: also great for MPI profiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/report1.png\" width=800px>\n",
    "\n",
    "(see `notebooks/backup/mpi_profiling_nsys`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Three ways to SAXPY on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAXPY** = **S**ingle precision **A** times **X** **P**lus **Y**\n",
    "\n",
    "‚Üí exercise **saxpy_gpu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/a100_saxpy_results.png\" width=1000px>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
