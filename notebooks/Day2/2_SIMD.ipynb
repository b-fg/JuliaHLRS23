{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMD stands for **\"Single Instruction Multiple Data\"** and falls into the category of instruction level **parallelism** (vector instructions). Since raw clock speeds haven't been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a \"vector\" (basically, a short sequence of values contiguous in memory).\n",
    "\n",
    "Consider this simple example where `A`, `B`, and `C` are vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_add(A, B, C)\n",
    "    for i in eachindex(A, B, C)\n",
    "        @inbounds A[i] = B[i] + C[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The idea behind SIMD is to perform the add instruction on multiple elements at the same time (instead of separately performing them one after another). The process of splitting up the simple loop addition into multiple vector additions is often denoted as \"loop vectorization\". Since each vectorized addition happens at instruction level, i.e. within a CPU core, the feature set of the CPU determines how many elements we can process in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/simd_vaddpd.png\" width=300px>\n",
    "<img src=\"./imgs/simd_register_width.png\" width=400px>\n",
    "\n",
    "(**Source:** Node-level performance engineering course by [NHR@FAU](https://hpc.fau.de/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check which \"advanced vector extensions\" (AVX) the system supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CpuId\n",
    "cpuinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(x -> contains(string(x), \"AVX\"), cpufeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hawk nodes do not have AVX512.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 2^16\n",
    "SIZE = 512^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(Float64, SIZE)\n",
    "B = rand(Float64, SIZE)\n",
    "C = rand(Float64, SIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_add(A,B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's not always so simple: SIMD can be hard..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autovectorization is a hard problem (it needs to prove a lot of things about the code!). After all, it is a from of parallelism and efficient parallelism can be hard as well...\n",
    "\n",
    "Not every loop is (readily) vectorizable. **Keep your loops as simple as possible!**\n",
    "\n",
    "* avoid conditionals and function calls etc.\n",
    "* ideally, loop length is static (countable up front).\n",
    "* access **contiguous data** (spatial locality).\n",
    "  * (align data structures to SIMD width boundary)\n",
    "* avoid data dependencies (e.g. between loop iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `vaddsd` instruction and usage of `xmmi` registers (128 bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How could this loop reduction be vectorized manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot_unrolled4(B, C)\n",
    "    a1 = zero(eltype(B))\n",
    "    a2 = zero(eltype(B))\n",
    "    a3 = zero(eltype(B))\n",
    "    a4 = zero(eltype(B))\n",
    "    @inbounds for i in 1:4:length(B)-4\n",
    "        a1 += B[i] * C[i]\n",
    "        a2 += B[i+1] * C[i+1]\n",
    "        a3 += B[i+2] * C[i+2]\n",
    "        a4 += B[i+3] * C[i+3]\n",
    "    end\n",
    "    return a1+a2+a3+a4\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_unrolled4(B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime vector_dot($B, $C);\n",
    "@btime vector_dot_unrolled4($B, $C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"automatic\" way: `@simd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To (try to) \"force\" automatic SIMD vectorization in Julia, you can use the `@simd` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot_simd(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    @simd for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `@simd` macro, we are **asserting several properties** of the loop:\n",
    "\n",
    "* It is safe to execute iterations in arbitrary or overlapping order, with special consideration for reduction variables.\n",
    "* Floating-point operations on reduction variables can be reordered, possibly causing different results than without `@simd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime vector_dot_simd($B, $C);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_simd(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `vfmadd231pd` instruction and usage of `ymmi` AVX registers (256 bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data types matter\n",
    "Floating-point addition is **non-associative** and the order of operations is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = rand(10^6)\n",
    "@show vector_dot(v,v);\n",
    "@show vector_dot_simd(v,v);\n",
    "@show abs(vector_dot(v,v) - vector_dot_simd(v,v));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How bad can this get? In principle, [arbitraily bad](https://discourse.julialang.org/t/when-shouldnt-we-use-simd/18276/11?u=carstenbauer)!! Quite often you can get away with it though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Compare this to integer addition, which is **associative** and the order of operations has no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_int = rand(Int64, SIZE);\n",
    "C_int = rand(Int64, SIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show vector_dot(B_int, C_int);\n",
    "@show vector_dot_simd(B_int, C_int);\n",
    "@show abs(vector_dot(B_int, C_int) - vector_dot_simd(B_int, C_int));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime vector_dot($B_int, $C_int);\n",
    "@btime vector_dot_simd($B_int, $C_int);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loosely speaking, you can think of `@turbo` as a much(!) more sophisticated version of `@simd`. Hopefully, these features will at some point just be part of Julia's compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sidenote: There is [LLVMLoopInfo.jl](https://github.com/JuliaSIMD/LLVMLoopInfo.jl) to talk to LLVM directly if necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LoopVectorization\n",
    "\n",
    "function vector_dot_turbo(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    @turbo for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end\n",
    "\n",
    "@btime vector_dot_simd($B, $C);\n",
    "@btime vector_dot_turbo($B, $C);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_turbo(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On systems with AVX512, `@turbo` will lead to usage of `zmmi` AVX512 512-bit registers! (**Not the case on Hawk which only has AVX2.**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: With `@turbo`, a basic matrix-matrix multiplication implementation can be **competitive to highly optimized BLAS**! (at least for small matrices where memory effects aren't important -> dMMM exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "X = zeros(N,N);\n",
    "Y = rand(N,N);\n",
    "Z = rand(N,N);\n",
    "\n",
    "using LoopVectorization\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra\n",
    "BLAS.set_num_threads(1)\n",
    "\n",
    "function mul_naive!(C, A, B)\n",
    "    for m in axes(A,1)\n",
    "        for n in axes(B,2)\n",
    "            Cmn = zero(eltype(C))\n",
    "            for k in axes(A,2)\n",
    "               @inbounds Cmn += A[m,k] * B[k,n]\n",
    "            end\n",
    "            C[m,n] = Cmn\n",
    "        end\n",
    "   end\n",
    "end\n",
    "\n",
    "function mul_turbo!(C, A, B)\n",
    "    @turbo for m in axes(A,1)\n",
    "        for n in axes(B,2)\n",
    "            Cmn = zero(eltype(C))\n",
    "            for k in axes(A,2)\n",
    "               @inbounds Cmn += A[m,k] * B[k,n]\n",
    "            end\n",
    "            C[m,n] = Cmn\n",
    "        end\n",
    "   end\n",
    "end\n",
    "\n",
    "@btime mul_naive!($X,$Y,$Z);\n",
    "@btime mul_turbo!($X,$Y,$Z);\n",
    "@btime mul!($X, $Y, $Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Array vs Array of Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data layout matters: contiguous memory access facilitates SIMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_numbers_aos = [rand() + im * rand() for i in 1:1024] # array of structs (Complex{Float64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: sum\n",
    "\n",
    "struct ComplexNumbers\n",
    "    x::Vector{Float64}\n",
    "    y::Vector{Float64}\n",
    "end\n",
    "\n",
    "sum(cn::ComplexNumbers) = sum(cn.x) + im * sum(cn.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_numbers_soa = ComplexNumbers(rand(1024), rand(1024)) # struct of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum($complex_numbers_aos);\n",
    "@btime sum($complex_numbers_soa);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [StructArrays.jl](https://github.com/JuliaArrays/StructArrays.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StructArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SoA = StructArray{Complex}((rand(1024), rand(1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum($SoA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@fastmath`\n",
    "\n",
    "Enables lots of floating point optimizations that are potentially *unsafe*! It trades accuracy for speed, so, [Beware of fast-math](https://simonbyrne.github.io/notes/fastmath/). (See the [LLVM Language Reference Manual](https://llvm.org/docs/LangRef.html#fast-math-flags) for more information on which compiler options it sets.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMD\n",
    "Among other things, it **facilitates SIMD vectorization** because it:\n",
    "* Allows re-association of operands in series of floating-point operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot_fastmath(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    @fastmath for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime vector_dot_fastmath($B, $C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_fastmath(B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMA - Fused Multiply Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b,c) = a*b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none f(1.0,2.0,3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fastmath(a,b,c) = @fastmath a*b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none f_fastmath(1.0,2.0,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In this specific case, the explicit `fma` function or [MuladdMacro.jl](https://github.com/SciML/MuladdMacro.jl) are *safer* alternatives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/skylake_microarchitecture.png\" width=700px>\n",
    "\n",
    "**Source:** [Intel® 64 and IA-32 Architectures Optimization Reference Manual](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidenote: Why doesn't Julia use FMA automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: because it can break math in weird ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(a,b,c)\n",
    "    @assert a*b ≥ c\n",
    "    return sqrt(a*b-c)\n",
    "end\n",
    "\n",
    "function f_fma(a,b,c)\n",
    "    @assert a*b ≥ c\n",
    "    return sqrt(fma(a,b,-c))\n",
    "end\n",
    "\n",
    "a = 1.0 + 0.5^27;\n",
    "b = 1.0 - 0.5^27;\n",
    "c = 1.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fma(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
