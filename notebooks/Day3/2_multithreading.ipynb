{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are threads?\n",
    "Threads are execution units within a process that can run simultaneously.\n",
    "\n",
    "<img src=\"./imgs/what-are-threads.png\" width=500px>\n",
    "\n",
    "While processes are entirely separate, threads run in a **shared memory** space (heap)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Julia with multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single *user thread*. We must tell it explicitly to start multiple user threads. There are two ways to do this:\n",
    "\n",
    "* Environment variable: `JULIA_NUM_THREADS=4`\n",
    "* Command line argument: `julia -t 4`\n",
    "\n",
    "**Jupyter lab:**\n",
    "\n",
    "The simplest way is to globally set the environment variable `JULIA_NUM_THREADS` (e.g. in the `.bashrc`). But one can also create a specific Jupyter kernel for multithreaded Julia:\n",
    "\n",
    "```julia\n",
    "using IJulia\n",
    "installkernel(\"Julia (4 threads)\", env=Dict(\"JULIA_NUM_THREADS\"=>\"4\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily check how many threads we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is currently not (easily) possible to change the number of threads at runtime!** (Will likely change in the future.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User threads vs default threads\n",
    "\n",
    "Technically, the Julia process is also spawning multiple threads already in \"single-threaded\" mode, like\n",
    "* a thread for unix signal listening\n",
    "* multiple OpenBLAS threads for BLAS/LAPACK operations\n",
    "\n",
    "For this reason, we call the threads specified via `-t` or the environment variable *user threads* or simply *Julia threads*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia waits for commands to finish (\"**blocking**\") and runs everything sequentially.\n",
    "\n",
    "**Tasks** are a feature that allows (parts of) computations to be scheduled (suspended and resumed) in a flexible manner to implement **concurrency** (multitasking) and **parallelism**.\n",
    "\n",
    "* Concurrency is about dealing with lots of things at once.\n",
    "* Parallelism is about doing lots of things at once .\n",
    "\n",
    "Example (concurrency): **asynchronous I/O** like\n",
    " * **multiple user input** (Why not already process some of the input?)\n",
    " * **data dumping to disk** (Maybe it's possible to continue a calculation?)\n",
    " * **receiving calculations from worker processes**\n",
    " \n",
    "Example (parallelism): **multithreading, distributed computing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@async` and `@sync`\n",
    "\n",
    "We can create a task for asynchronous execution with the [`@async` macro](https://docs.julialang.org/en/v1/base/parallel/#Base.@async). What this means is that for whatever falls into its scope, Julia will start a task to then proceed to whatever comes next in the script without waiting for the task to complete (\"**non-blocking**\").\n",
    "\n",
    "(**Note:** `@async` is kind of deprecated in favor of `@spawn` below, but we quickly mention it here nonetheless for pedagogical reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time sleep(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @async sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia allows the script to proceed (and the `@time` macro to fully execute) without waiting for the task (in this case, sleeping for two seconds) to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the partner macro `@sync` to synchronize, that is wait for all encapsulated tasks. (see `?@sync`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @sync @async sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, here it doesn't make much sense to write `@sync @async` - we could simply drop it altogether. A better example is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @sync begin\n",
    "    @async sleep(2.0)\n",
    "    @async sleep(2.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(1000,1000)\n",
    "B = rand(1000,1000)\n",
    "\n",
    "t = @async A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-based multithreading\n",
    "\n",
    "In traditional HPC, we typically care about threads directly. Using e.g. OpenMP, we essentially tell each thread what to do.\n",
    "\n",
    "Conceptually, Julia takes a different approach and implements **task-based** multithreading. In this paradigm, a task - e.g. a computational piece of a code - is marked for **parallel** execution on **any** of the available Julia threads. Julias **dynamic scheduler** will automatically put the task on one of the threads and trigger the execution of the task on said thread.\n",
    "\n",
    "**Users should think about tasks and not threads**.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/task-based-parallelism.png\" width=250px>\n",
    "<br>\n",
    "\n",
    "**Advantages:**\n",
    "* high-level abstraction\n",
    "* **composability / nestability** (Multithreaded code can call multithreaded code can call multithreaded code ....)\n",
    "\n",
    "**Disadvantages:**\n",
    "* potential scheduling overhead\n",
    "* task → thread assignment uncertain (can vary dynamically + task migration)\n",
    "* **can get in the way when performance engineering**\n",
    "  * scheduler has limited information (e.g. about the system topology)\n",
    "  * low-level profiling (e.g. with LIKWID) requires fixed `task -> thread -> core` mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawning tasks on threads: `Threads.@spawn`\n",
    "`Threads.@spawn` spawns a task on a Julia thread. Specifically, it creates (and immediately returns) a `Task` and schedules it for execution on an available Julia thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to prefix `Threads.` to `@spawn` (and other threading-related functions) let's load everything from `Base.Threads` into global scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn println(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Threads.@spawn` returns the task right away - it is **non-blocking** - the result might only be fetchable after some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = @spawn begin\n",
    "    sleep(3);\n",
    "    \"result\"\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use (some of) the control flow tools that we've already covered, like `@sync`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync t = @spawn begin\n",
    "    sleep(3);\n",
    "    \"result\"\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:2*nthreads()\n",
    "    @spawn println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Recursive Fibonacci series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F(n) = F(n-1) + F(n-2), \\qquad F(1) = F(2) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can nest `@spawn` calls freely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fib(n)\n",
    "    n < 2 && return n\n",
    "    t = @spawn fib(n-2)\n",
    "    return fib(n-1) + fetch(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fib.(1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: Algorithmically, this is a highly inefficient implementation of the Fibonacci series, of course!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tmap` (threaded `map`)\n",
    "\n",
    "(again, not the most efficient implementation but fine for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(fn, itr) = map(fetch, map(i -> Threads.@spawn(fn(i)), itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [rand(200,200) for i in 1:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(i -> println(i, \" ($(threadid()))\"), 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that this implementation creates temporary allocations and thus isn't particularly efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime tmap($svdvals, $M);\n",
    "@btime map($svdvals, $M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load-balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are many tasks (e.g. many more than there are threads), Julia's scheduler balances the load of these tasks among threads. (**non-uniform workloads**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_nonuniform_spawn!(a, niter = zeros(Int, nthreads()), load = zeros(Int, nthreads()))\n",
    "    @sync for i in 1:length(a)\n",
    "        Threads.@spawn begin\n",
    "            a[i] = sum(abs2, rand() for j in 1:i)\n",
    "            \n",
    "            # poor-mans bookkeeping (unsafe!)\n",
    "            niter[threadid()] += 1\n",
    "            load[threadid()] += i\n",
    "        end\n",
    "    end\n",
    "    return niter, load\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "a = zeros(nthreads()*10_000)\n",
    "niter, load = compute_nonuniform_spawn!(a)\n",
    "\n",
    "b1 = bar(niter, xlab=\"threadid\", ylab=\"# iterations\", title=\"Number of iterations (@spawn)\", legend=false)\n",
    "b2 = bar(load, xlab=\"threadid\", ylab=\"workload\", title=\"Workload (@spawn)\", legend=false, color=:green)\n",
    "\n",
    "display(b1)\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading for-loops: `@threads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads for i in 1:2*nthreads()\n",
    "    println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `@threads` creates `nthreads()` many tasks each processing a contigious region of the iteration space. Each task is then essentially spawned with `@spawn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "function square!(x)\n",
    "    for i in eachindex(x)\n",
    "        x[i] = x[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "function square_threads!(x)\n",
    "    @threads for i in eachindex(x)\n",
    "        x[i] = x[i]^2\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(1_000_000)\n",
    "@btime square!($x);\n",
    "@btime square_threads!($x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nestability / Composability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreaded loops can be nested! → composability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function square_threads_all!(xs)\n",
    "    @threads for i in eachindex(xs)\n",
    "        square_threads!(xs[i])\n",
    "    end\n",
    "end\n",
    "function square_all!(xs)\n",
    "    @threads for i in eachindex(xs)\n",
    "        square!(xs[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [rand(n,n) for n in (100,1000,10_000)]\n",
    "\n",
    "@btime square_threads_all!($xs) samples = 5 evals = 3\n",
    "@btime square_all!($xs) samples = 5 evals = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-based vs thread-based multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one is coming from an OpenMP background (or similar), it is very easy to not forget about the task-based nature of Julia's multithreading. This might even be reinforced by names like `@threads` and the existence of functions like `threadid()`. Unfortunately, this can readily lead to incorrect code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task migration and `threadid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's scheduler isn't only dynamically assigning tasks to any of the Julia threads, but it is also free to **migrate tasks between threads**. For example, a task might start running on Julia thread 1, then be paused and moved to Julia thread 3, where it then finishes execution. Hence, by default, there is **no fixed task-thread mapping**.\n",
    "\n",
    "→ **`threadid()` should be used with extreme care** as its output isn't guaranteed to be constant across the exectution of a task!\n",
    "\n",
    "(Also, `@threads` by default doesn't make many (long-time) guarantees about how iterations get mapped to tasks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problematic example: parallel summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_unsafe(data)\n",
    "    psums = zeros(nthreads())\n",
    "    @threads for x in data\n",
    "        psums[threadid()] += x\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this conceptually unsafe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while semantically unsafe, the function above might still work fine in practice. This is because task migration is (at least as of now) very rare (and, on a technical note, the above loop iterations don't yield)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rand(1_000 * nthreads());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_threads_unsafe(data) ≈ sum(data) # almost certainly still gives true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix 1: Chunking\n",
    "\n",
    "**Idea:** Partition the data into chunks and then iterate over chunk indices instead of the data itself. In each iteration (task) one chunk is processed. (Below the number of chunks is chosen as `nthreads()`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_chunks(data)\n",
    "    psums = zeros(nthreads())\n",
    "    \n",
    "     # manual partitioning of data\n",
    "    data_chunks = collect(Iterators.partition(data, length(data)÷nthreads()))\n",
    "    \n",
    "    @threads for tid in 1:nthreads() # iterate over chunk/thread ids\n",
    "        for x in data_chunks[tid] # iterate over data chunk\n",
    "            psums[tid] += x # tid is safe because it is constant across one iteration\n",
    "        end\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The **iteration variable** is always constant across one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_threads_chunks(data) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package [ChunkSplitters.jl](https://github.com/m3g/ChunkSplitters.jl) simplifies this pattern of manual chunking a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ChunkSplitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect(chunks(data, nthreads()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_chunksplitters(data; nchunks=nthreads())\n",
    "    psums = zeros(nchunks)\n",
    "    @threads for (data_range, ichunk) in chunks(data, nchunks)\n",
    "        for idata in data_range\n",
    "            psums[ichunk] += data[idata]\n",
    "        end\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_threads_chunksplitters(data) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this chunking scheme also isn't \"thread-biased\" anymore in the sense that we can choose `nchunks != nthreads()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix 2: Opt-out of task migration (*sticky tasks*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose the `:static` scheduling option for `@threads` to opt-out of Julia's dynamic scheduling and get **guarantees about the task-thread assignment** (and the iterations → task mapping). Clean solution for \"traditional HPC\", where one wants to think about threads at a low-level, but non-composable etc.\n",
    "\n",
    "\n",
    "Syntax: `@threads :static for ...`\n",
    "\n",
    " * splits up the iteration space into `nthreads()` even, contiguous blocks (in-order) and creates precisely one task per block\n",
    " * **statically** maps tasks to threads, specifically: task 1 -> thread 1, task 2 -> thread 2, etc.\n",
    "   * -> no task migration, i.e. **fixed task-thread mapping** 👍\n",
    "   * -> only little overhead 👍\n",
    "   * -> not composable / nestable 👎\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:2*nthreads()\n",
    "    println(i, \" -> thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:2*nthreads()\n",
    "    println(i, \" -> thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `@threads :static`, every thread handles precisely two iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:3\n",
    "    @threads :dynamic for j in 1:3\n",
    "        println(\"$i, $j\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:3\n",
    "    @threads :static for j in 1:3\n",
    "        println(\"$i, $j\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about `@spawn`**: we can also opt-out of task migration for `@spawn` and **spawn *sticky* tasks on specific threads**. However, Julia doesn't have a built-in tool for this (as of now) and one needs to use a package like, e.g., [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl). The latter exports a function `@tspawnat <threadid> ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ThreadPinning\n",
    "\n",
    "@tspawnat 2 println(\"running on thread \", threadid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No load-balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, `@threads` doesn't give load-balancing (with any scheduler) as it only starts `nthreads()` tasks in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_nonuniform_threads!(a, niter = zeros(Int, nthreads()), load = zeros(Int, nthreads()))\n",
    "    @threads for i in 1:length(a)\n",
    "        a[i] = sum(abs2, rand() for j in 1:i)\n",
    "\n",
    "        # poor-mans bookkeeping (unsafe!)\n",
    "        niter[threadid()] += 1\n",
    "        load[threadid()] += i\n",
    "    end\n",
    "    return niter, load\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "a = zeros(nthreads()*10_000)\n",
    "niter, load = compute_nonuniform_threads!(a)\n",
    "\n",
    "b1 = bar(niter, xlab=\"threadid\", ylab=\"# iterations\", title=\"Number of iterations (@threads)\", legend=false)\n",
    "b2 = bar(load, xlab=\"threadid\", ylab=\"workload\", title=\"Workload (@threads)\", legend=false, color=:green)\n",
    "\n",
    "display(b1)\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(There will likely be a scheduling option for `@threads` that implements load-balancing in the future.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading: Things to be aware of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data races and thread safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_serial(x)\n",
    "    s = zero(eltype(x))\n",
    "    for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_naive(x)\n",
    "    s = zero(eltype(x))\n",
    "    @threads for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = rand(nthreads()*10_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(numbers);\n",
    "@show sum_serial(numbers);\n",
    "@show sum_threads_naive(numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong** result! Even worse, it's **non-deterministic** and different every time! It's also slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_serial($numbers);\n",
    "@btime sum_threads_naive($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: There is a [race condition](https://en.wikipedia.org/wiki/Race_condition).\n",
    "\n",
    "Note that race conditions aren't specific to reductions. More generally, they can appear when multiple tasks are modifying a shared \"global\" state simultaneously.\n",
    "\n",
    "Not all of Julia and its packages in the ecosystem are thread-safe! In general, it is safer to assume that they're not unless documented/proven otherwise. (Example: `Dict` isn't thread-safe!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix 1: Chunking (divide the work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_chunksplitters(x; nchunks=nthreads())\n",
    "    psums = zeros(eltype(x), nchunks)\n",
    "    @threads for (data_range, ichunk) in chunks(x, nchunks)\n",
    "        for idata in data_range\n",
    "            @inbounds psums[ichunk] += x[idata]\n",
    "        end\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(numbers);\n",
    "@show sum_serial(numbers);\n",
    "@show sum_threads_chunksplitters(numbers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_threads_chunksplitters($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speedup and correct result!\n",
    "\n",
    "\n",
    "**Note: [False sharing](https://en.wikipedia.org/wiki/False_sharing#:~:text=In%20computer%20science%2C%20false%20sharing,managed%20by%20the%20caching%20mechanism.)**\n",
    "\n",
    "`sum_threads_cunksplitters` above still has a more subtle performance issue because different tasks mutate non-local state (`psums`) in parallel. There is no (obvious) data race because they access different slots (`ichunk`). However, CPU cores work with cache **lines** instead of single elements.\n",
    "\n",
    "Different tasks modify the same cache line → need for synchronization → performance decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible solution: Entirely **task-local** computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_chunks_tasklocal(x)\n",
    "    chunks = Iterators.partition(x, length(x) ÷ Threads.nthreads())\n",
    "    tasks = map(chunks) do chunk\n",
    "        Threads.@spawn sum(chunk)\n",
    "    end\n",
    "    chunk_sums = fetch.(tasks)\n",
    "    return sum(chunk_sums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* no non-local mutation\n",
    "* each task computes a partial sum independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_threads_chunks_tasklocal($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix 2 (not recommended): Low-level atomics and locks\n",
    "\n",
    "See [Atomic Operations](https://docs.julialang.org/en/v1/manual/multi-threading/#Atomic-Operations) and/or [Data-race freedom](https://docs.julialang.org/en/v1/manual/multi-threading/#Data-race-freedom) in the Julia doc for more information. In general, one should avoid using them as much as possible since they actually limit the parallelism (especially if you don't know what you're doing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage collection\n",
    "\n",
    "[As of now](https://www.youtube.com/watch?v=Ks0p6PQyIPs), **Julia's GC is not parallel** and doesn't work nicely with multithreading. (Update: In Julia 1.10 the GC will be parallel!)\n",
    "\n",
    "If it gets triggered, it stops the world (all threads) for clearing up memory.\n",
    "\n",
    "Hence, when using multithreading, it is even more important to **avoid heap allocations!**\n",
    "\n",
    "(If you can't avoid allocations, consider using multiprocessing instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level tools for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl)\n",
    "\n",
    "*Parallelized Base functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ThreadsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreadsX.sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime ThreadsX.sum($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [FLoops.jl](https://github.com/JuliaFolds/FLoops.jl)\n",
    "\n",
    "*Fast sequential, threaded, and distributed for-loops for Julia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using FLoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_floops(x)\n",
    "    @floop for xi in x\n",
    "        @reduce(s = zero(eltype(x)) + xi)\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_floops($numbers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = rand(nthreads()*10_000);\n",
    "\n",
    "sum_floops(numbers) ≈ sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_serial($numbers);\n",
    "@btime sum_floops($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@floop` supports different *executors* that allow for easy switching between serial and threaded execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_floops(x, executor)\n",
    "    @floop executor for xi in x\n",
    "        @reduce(s += xi)\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_floops($numbers, $(SequentialEx()));\n",
    "@btime sum_floops($numbers, $(ThreadedEx()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more [executors](https://juliafolds.github.io/FLoops.jl/stable/tutorials/parallel/#tutorials-executor), like `DistributedEx` or `CUDAEx`. See, e.g., [FoldsThreads.jl](https://github.com/JuliaFolds/FoldsThreads.jl) and [FoldsCUDA.jl](https://github.com/JuliaFolds/FoldsCUDA.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, FLoops is built on top of [Transducers.jl](https://juliafolds.github.io/Transducers.jl/stable/tutorials/tutorial_parallel/) (i.e. it translates for-loop semantics into folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tullio.jl](https://github.com/mcabbott/Tullio.jl)\n",
    "\n",
    "*Tullio is a very flexible einsum macro* ([Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tullio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(10,10)\n",
    "B = rand(10,10)\n",
    "\n",
    "C = @tullio C[i,j] := A[i,k] * B[k,j] # matrix multiplication\n",
    "\n",
    "C ≈ A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tullio(xs) = @tullio S := xs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_tullio($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Uses `fastmath` and other tricks to be faster here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "\n",
    "*Macro(s) for vectorizing loops.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LoopVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_turbo(x)\n",
    "    s = zero(eltype(x))\n",
    "    @tturbo for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_turbo($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Uses all kinds of SIMD tricks to be faster than the others.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread affinity/pinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compute node has a complex topology (two sockets, multiple memory channels/domains). **It matters (dramatically) for performance where your Julia threads are running!** → Thread pinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hawk compute node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/lstopo_hawk.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning Julia threads to CPU threads/cores\n",
    "\n",
    "What about external tools like `numactl`, `taskset`, etc.? Doesn't work reliably because they often [can't distinguish](https://discourse.julialang.org/t/thread-affinitization-pinning-julia-threads-to-cores/58069/5) between Julia threads and other internal threads.\n",
    "\n",
    "**Options:**\n",
    "\n",
    "* Environment variable: `JULIA_EXCLUSIVE=1` (compact pinning)\n",
    "* More control and convenient visualization: [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl) -> **Exercise saxpy_cpu**\n",
    "\n",
    "#### [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl)\n",
    "\n",
    "(See my short talk at JuliaCon2023 @ MIT: https://youtu.be/6Whc9XtlCC0)\n",
    "\n",
    "**Pinning at three conceptual levels**\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/threadpinning_pinthreads.svg\" width=600px>\n",
    "<br>\n",
    "\n",
    "* `:cputhreads:` pin to CPU threads (incl. \"hypterthreads\") one after another\n",
    "* `:cores:` pin to CPU cores one after another\n",
    "* `:numa:` alternate between NUMA domains so, e.g., 0, 16, 32, 48, 64, .... (if a NUMA domain has 16 cores)\n",
    "* `:sockets:` alternate between sockets so, e.g., 0, 64, 1, 65, 2, 66, .... (if a socket has 64 cores)\n",
    "\n",
    "\n",
    "**Visualization of cluster topology and thread affinities**\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/threadinfo.png\" width=1000px>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
