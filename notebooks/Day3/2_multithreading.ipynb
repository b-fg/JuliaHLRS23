{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are threads?\n",
    "Threads are execution units within a process that can run simultaneously.\n",
    "\n",
    "<img src=\"./imgs/processes_threads.png\" width=400px>\n",
    "\n",
    "While processes are entirely separate, threads run in a **shared memory** space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Julia with multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single *user thread*. We must tell it explicitly to start multiple user threads. There are two ways to do this:\n",
    "\n",
    "* Environment variable: `JULIA_NUM_THREADS=4`\n",
    "* Command line argument: `julia -t 4`\n",
    "\n",
    "**Jupyter lab:**\n",
    "\n",
    "The simplest way is to globally set the environment variable `JULIA_NUM_THREADS` (e.g. in the `.bashrc`). But one can also create a specific Jupyter kernel for multithreaded Julia:\n",
    "\n",
    "```julia\n",
    "using IJulia\n",
    "installkernel(\"Julia (4 threads)\", env=Dict(\"JULIA_NUM_THREADS\"=>\"4\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily check how many threads we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User threads vs default threads\n",
    "\n",
    "Technically, the Julia process is also spawning multiple threads already in \"single-threaded\" mode, like\n",
    "* a thread for unix signal listening\n",
    "* multiple OpenBLAS threads for BLAS/LAPACK operations\n",
    "\n",
    "For this reason, we call the threads specified via `-t` or the environment variable *user threads* or simply *Julia threads*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia waits for commands to finish (\"**blocking**\") and runs everything sequentially.\n",
    "\n",
    "**Tasks** are a feature that allows (parts of) computations to be scheduled (suspended and resumed) in a flexible manner to implement **concurrency** (multitasking) and **parallelism**.\n",
    "\n",
    "* Concurrency is about dealing with lots of things at once.\n",
    "* Parallelism is about doing lots of things at once .\n",
    "\n",
    "Example (concurrency): **asynchronous I/O** like\n",
    " * **multiple user input** (Why not already process some of the input?)\n",
    " * **data dumping to disk** (Maybe it's possible to continue a calculation?)\n",
    " * **receiving calculations from worker processes**\n",
    " \n",
    "Example (parallelism): **multithreading, distributed computing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@async` and `@sync`\n",
    "\n",
    "We can create a task for asynchronous execution with the [`@async` macro](https://docs.julialang.org/en/v1/base/parallel/#Base.@async). What this means is that for whatever falls into its scope, Julia will start a task to then proceed to whatever comes next in the script without waiting for the task to complete (\"**non-blocking**\").\n",
    "\n",
    "(**Note:** `@async` is kind of deprecated in favor of `@spawn` below, but we quickly mention it here nonetheless for pedagogical reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.003316 seconds (66 allocations: 1.688 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time sleep(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.009963 seconds (7.35 k allocations: 448.450 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x0000000112f35f90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time @async sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia allows the script to proceed (and the `@time` macro to fully execute) without waiting for the task (in this case, sleeping for two seconds) to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the partner macro `@sync` to synchronize, that is wait for all encapsulated tasks. (see `?@sync`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.037175 seconds (8.27 k allocations: 563.504 KiB, 1.57% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Task (done) @0x0000000113085e40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time @sync @async sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, here it doesn't make much sense to write `@sync @async` - we could simply drop it altogether. A better example is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.017607 seconds (15.93 k allocations: 1.073 MiB, 0.79% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Task (done) @0x000000011328c940"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time @sync begin\n",
    "    @async sleep(2.0)\n",
    "    @async sleep(2.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (done) @0x00000001108c0160"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(1000,1000)\n",
    "B = rand(1000,1000)\n",
    "\n",
    "t = @async A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Matrix{Float64}:\n",
       " 258.863  251.988  251.66   248.055  …  254.089  258.598  252.147  251.962\n",
       " 256.655  246.754  252.633  248.102     255.455  252.97   255.976  246.44\n",
       " 247.152  241.599  241.174  241.047     244.05   246.678  246.611  236.804\n",
       " 261.366  255.333  257.759  250.723     253.528  262.9    263.362  251.323\n",
       " 251.187  248.668  251.973  242.18      248.841  239.106  248.397  241.887\n",
       " 253.758  251.199  255.334  241.883  …  254.323  250.681  251.337  244.613\n",
       " 246.54   251.146  249.127  241.508     252.774  246.718  246.471  244.15\n",
       " 251.284  248.965  242.299  239.541     247.846  251.664  252.859  242.705\n",
       " 250.135  245.174  246.241  243.463     243.931  254.803  250.343  247.529\n",
       " 257.451  251.01   250.736  241.272     250.654  252.469  254.773  252.831\n",
       " 238.57   240.734  235.473  230.755  …  239.8    234.493  240.531  230.98\n",
       " 254.714  243.69   248.54   240.839     246.479  249.655  252.558  245.778\n",
       " 252.709  246.558  245.182  247.438     248.51   249.056  249.118  244.268\n",
       "   ⋮                                 ⋱                             \n",
       " 252.214  243.976  243.823  241.533     244.394  244.782  247.731  241.882\n",
       " 255.054  248.255  245.265  245.278     250.167  255.348  253.959  248.043\n",
       " 259.458  251.119  253.843  245.901  …  255.199  253.028  255.146  253.279\n",
       " 261.946  264.477  257.249  252.174     265.045  257.315  265.721  255.337\n",
       " 254.593  252.184  253.677  246.505     256.21   254.106  252.945  246.127\n",
       " 263.665  254.229  250.864  248.384     258.22   247.699  259.428  248.795\n",
       " 256.311  245.821  250.563  241.786     253.717  254.945  250.064  248.164\n",
       " 243.983  240.446  243.268  236.502  …  247.745  242.026  245.545  240.194\n",
       " 252.783  239.749  243.914  243.967     242.22   244.755  244.483  242.134\n",
       " 246.184  240.153  234.77   232.356     244.885  240.638  243.584  241.281\n",
       " 256.097  252.008  252.243  244.519     253.951  256.072  253.583  247.036\n",
       " 245.893  245.23   250.881  243.318     245.409  241.504  244.439  238.241"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-based multithreading\n",
    "\n",
    "In traditional HPC, we typically care about threads directly. Using e.g. OpenMP, we essentially tell each thread what to do.\n",
    "\n",
    "Conceptually, Julia takes a different approach and implements **task-based** multithreading. In this paradigm, a task - e.g. a computational piece of a code - is marked for **parallel** execution on **any** of the available Julia threads. Julias **dynamic scheduler** will automatically put the task on one of the threads and trigger the execution of the task on said thread.\n",
    "\n",
    "Ideally, **a user should think about tasks and not threads**.\n",
    "\n",
    "**Advantages:**\n",
    "* high-level and convenient\n",
    "* **composability / nestability** (Multithreaded code can call multithreaded code can call multithreaded code ....)\n",
    "\n",
    "**Disadvantages:**\n",
    "* potential scheduling overhead\n",
    "* **can get in the way when performance engineering**\n",
    "  * scheduler has limited information (e.g. about the system topology)\n",
    "  * low-level profiling (e.g. with LIKWID) currently requires a known task -> thread -> cpu core mapping.\n",
    "\n",
    "(Blog post: [Announcing composable multi-threaded parallelism in Julia](https://julialang.org/blog/2019/07/multithreading/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawning tasks on threads: `Threads.@spawn`\n",
    "`Threads.@spawn` spawns a task on a Julia thread. Specifically, it creates (and immediately returns) a `Task` and schedules it for execution on an available Julia thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to prefix `Threads.` to `@spawn` (and other threading-related functions) let's load everything from `Base.Threads` into global scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x00000001132aa8c0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "@spawn println(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Threads.@spawn` returns the task right away - it is **non-blocking** - the result might only be fetchable after some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.983542 seconds (95 allocations: 2.547 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"result\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = @spawn begin\n",
    "    sleep(3);\n",
    "    \"result\"\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use (some of) the control flow tools that we've already covered, like `@sync`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000004 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"result\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sync t = @spawn begin\n",
    "    sleep(3);\n",
    "    \"result\"\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:2*nthreads()\n",
    "    @spawn println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Recursive Fibonacci series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F(n) = F(n-1) + F(n-2), \\qquad F(1) = F(2) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can nest `@spawn` calls freely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fib (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fib(n)\n",
    "    n < 2 && return n\n",
    "    t = @spawn fib(n-2)\n",
    "    return fib(n-1) + fetch(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Vector{Int64}:\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  5\n",
       "  8\n",
       " 13\n",
       " 21\n",
       " 34\n",
       " 55"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Hi, I'm 6\n",
      "Hi, I'm 6\n",
      "Hi, I'm 1\n",
      "Hi, I'm 3\n",
      "Hi, I'm 6\n",
      "Hi, I'm 6\n",
      "Hi, I'm 6\n",
      "Hi, I'm 6\n",
      "Hi, I'm 2\n",
      "Hi, I'm 6\n",
      "Hi, I'm 4\n"
     ]
    }
   ],
   "source": [
    "fib.(1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: Algorithmically, this is a highly inefficient implementation of the Fibonacci series, of course!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tmap` (threaded `map`)\n",
    "\n",
    "(again, not the most efficient implementation but fine for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmap (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmap(fn, itr) = map(fetch, map(i -> Threads.@spawn(fn(i)), itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [rand(200,200) for i in 1:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Vector{Float64}}:\n",
       " [100.22170255091726, 8.24052689368954, 7.920930771051425, 7.753311895941921, 7.662172505595194, 7.566895404062618, 7.491885588995352, 7.429194837557763, 7.333251767465407, 7.295327268695022  …  0.2589182851822991, 0.23679912303817682, 0.21108330393241795, 0.19554370286798717, 0.165883670293, 0.12749719318923072, 0.10676202595374887, 0.09115153599396487, 0.054574455869891195, 0.02642592857816373]\n",
       " [99.82297643141749, 8.000913007213283, 7.883720528251313, 7.756708339652432, 7.71526567248039, 7.5986296018015596, 7.510962866843324, 7.4605892912946805, 7.28255516167145, 7.232648003920655  …  0.334930969474883, 0.2710295768435623, 0.2637360301764507, 0.21983630641359803, 0.1981284648151442, 0.15540874909087712, 0.10733525370730641, 0.048014330373218686, 0.029627805895823492, 0.006789924306356225]\n",
       " [100.19897835581133, 7.91330160167902, 7.839735145235196, 7.800795447444994, 7.631962190125877, 7.562194445720498, 7.514048943814794, 7.333581657411916, 7.259053053495001, 7.2349632330741676  …  0.33919066482813304, 0.3308970874896036, 0.2613186953547644, 0.21850420207093918, 0.19999459075669074, 0.1647296105842593, 0.1338257590050939, 0.07763658461918782, 0.06157038768731236, 3.773298556501305e-5]\n",
       " [99.99602579889776, 7.926378537760099, 7.791619326717144, 7.624402711409495, 7.5903883548328945, 7.456336759722712, 7.382710474788745, 7.35758714318189, 7.296159527752827, 7.178802196661384  …  0.31419967451467645, 0.28098809685546317, 0.21420760612551631, 0.2064792807842247, 0.1860183468132572, 0.1395192551227548, 0.11660526090635813, 0.061387221086872265, 0.029735420140970616, 0.010108507458660689]\n",
       " [99.95385789287374, 7.988498242822348, 7.7489669608762055, 7.667038081566945, 7.656156549880213, 7.537929848974175, 7.465588289907027, 7.3279379305436985, 7.289988102140514, 7.174867152092066  …  0.33435020390626663, 0.3013709066865929, 0.27629701868436335, 0.21884486904060182, 0.19149854623443138, 0.13962959769815322, 0.11423748878269274, 0.0875405691472218, 0.06438076741476827, 0.04630391737772015]\n",
       " [100.1915875839298, 8.031440329787495, 7.973222241397189, 7.811706602350762, 7.737494030730236, 7.6757384198604965, 7.487511817935593, 7.446561195235687, 7.36973710277588, 7.249439098752569  …  0.28648430148424503, 0.26559461276582846, 0.2506001361301595, 0.19905071335531926, 0.1585079899731294, 0.12030301016285512, 0.10224808822064661, 0.08668732192230334, 0.07451448293281093, 0.03175213053787448]\n",
       " [99.58489238674296, 8.05554014359874, 7.938747701154756, 7.722541288832384, 7.6947516253181965, 7.5182668024496335, 7.435254301249899, 7.355971104858992, 7.329013264377877, 7.152753192695934  …  0.28929240937560613, 0.26976407117259293, 0.25326930645203843, 0.21832956437812792, 0.1648338568994019, 0.14642663060361683, 0.12018505764132575, 0.08357295432387096, 0.06209191565900412, 0.025269649014880595]\n",
       " [100.79721092295497, 8.068630975483234, 7.858101734900402, 7.750826212891163, 7.6229670475175055, 7.618892808232285, 7.473416466891804, 7.4416075485963, 7.321556033593647, 7.293829493954241  …  0.2956924629042539, 0.28383650120185216, 0.2533001730315552, 0.21387983596040888, 0.20162671753127803, 0.15080398687728078, 0.1086901812191208, 0.06570811799594042, 0.03557020109821818, 0.006246828974154007]\n",
       " [100.60985564811608, 7.986180892817622, 7.834672389424967, 7.780524086942947, 7.706726785078698, 7.572715072474117, 7.496925924503034, 7.457326067753413, 7.402936829049533, 7.353110858036367  …  0.280641698603669, 0.2495490844782787, 0.21525809250056094, 0.19803946552318136, 0.16651468297889638, 0.12430832254096623, 0.09111470636680259, 0.06808775411052126, 0.029209393442899707, 0.019861898730724897]\n",
       " [100.37163670552434, 8.065584293162566, 7.864257782786388, 7.728725835216261, 7.6831393086031685, 7.606277919592922, 7.440301186029368, 7.3335608420143785, 7.275942179475993, 7.248609855278818  …  0.32737698608930754, 0.30147988266024683, 0.27494362656870314, 0.20389413340779738, 0.15364649164617256, 0.14226878544377988, 0.10539964739894894, 0.07272869275413804, 0.05454374165974915, 0.01841253463656282]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 (5)\n",
      "3 (5)\n",
      "7 (2)\n",
      "8 (3)\n",
      "2 (5)\n",
      "5 (4)\n",
      "1 (5)\n",
      "10 (6)\n",
      "6 (5)\n",
      "4 (1)\n"
     ]
    }
   ],
   "source": [
    "tmap(i -> println(i, \" ($(threadid()))\"), 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that this implementation creates temporary allocations and thus isn't particularly efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20.900 ms (148 allocations: 4.22 MiB)\n",
      "  53.099 ms (81 allocations: 4.22 MiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime tmap($svdvals, $M);\n",
    "@btime map($svdvals, $M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading for-loops: `@threads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm 2\n",
      "Hi, I'm 6\n",
      "Hi, I'm 3\n",
      "Hi, I'm 4\n",
      "Hi, I'm 1\n",
      "Hi, I'm 3\n",
      "Hi, I'm 4\n",
      "Hi, I'm 2\n",
      "Hi, I'm 2\n",
      "Hi, I'm 6\n",
      "Hi, I'm 5\n",
      "Hi, I'm 4\n"
     ]
    }
   ],
   "source": [
    "@threads for i in 1:2*nthreads()\n",
    "    println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `@threads` creates `nthreads()` many tasks each processing a contigious region of the iteration space. Each task is then essentially spawned with `@spawn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  302.639 μs (0 allocations: 0 bytes)\n",
      "  76.121 μs (32 allocations: 2.81 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "function square!(x)\n",
    "    for i in eachindex(x)\n",
    "        x[i] = x[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "function square_threads!(x)\n",
    "    @threads for i in eachindex(x)\n",
    "        x[i] = x[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "x = rand(1_000_000)\n",
    "@btime square!($x);\n",
    "@btime square_threads!($x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-based vs thread-based multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one is coming from an OpenMP background (or similar), it is very easy to not consider the task-based nature of Julia's multithreading. This might even be reinforced by names like `@threads` and the existence of functions like `threadid()`. Unfortunately, this can readily lead to incorrect code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task migration and `threadid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the user should conceptually only care about tasks, Julia's scheduler isn't only dynamically assigning tasks to any of the Julia threads, but it is also free to **migrate tasks between threads**. For example, a task might start running on Julia thread 1, then be paused and moved to Julia thread 3, where it then finishes execution. Hence, by default, there is **no fixed task-thread mapping**.\n",
    "\n",
    "→ **`threadid()` should be used with extreme care** as its output isn't guaranteed to be constant across the exectution of a task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unsafe (!) example: partial sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partial_sums_unsafe (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function partial_sums_unsafe(data)\n",
    "    psums = zeros(nthreads())\n",
    "    @threads for x in data\n",
    "        tid = threadid()\n",
    "        old_sum = psums[tid]\n",
    "        new_sum = old_sum + x\n",
    "        psums[tid] = new_sum\n",
    "    end\n",
    "    return psums\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this conceptually unsafe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while semantically unsafe, the function above might still work fine in practice. This is because task migration is (at least as of now) very rare. (The scheduler isn't using the freedom much.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rand(1_000 * nthreads());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(partial_sums_unsafe(data)) ≈ sum(data) # very likely still gives true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to fix the issue?\n",
    "\n",
    "* **Option 1:** Iterate over (e.g. thread) indices instead of data. Then use the **loop variable** (which is constant across one iteration) to process chunks of the data in each iteration (task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partial_sums_safe1 (generic function with 1 method)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function partial_sums_safe1(data)\n",
    "    psums = zeros(nthreads())\n",
    "    \n",
    "     # manual partitioning of data\n",
    "    data_chunks = collect(Iterators.partition(data, length(data)÷nthreads()))\n",
    "    \n",
    "    @threads for tid in 1:nthreads() # iterate over thread ids\n",
    "        for x in data_chunks[tid] # iterate over data chunk\n",
    "            old_sum = psums[tid]\n",
    "            new_sum = old_sum + x\n",
    "            psums[tid] = new_sum\n",
    "        end\n",
    "    end\n",
    "    return psums\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(partial_sums_safe1(data)) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package [ChunkSplitters.jl](https://github.com/m3g/ChunkSplitters.jl) simplifies this pattern of manual partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ChunkSplitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Tuple{UnitRange{Int64}, Int64}}:\n",
       " (1:1000, 1)\n",
       " (1001:2000, 2)\n",
       " (2001:3000, 3)\n",
       " (3001:4000, 4)\n",
       " (4001:5000, 5)\n",
       " (5001:6000, 6)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(chunks(data, nthreads()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partial_sums_safe_chunks (generic function with 1 method)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function partial_sums_safe_chunks(data; nchunks=nthreads())\n",
    "    psums = zeros(nchunks)\n",
    "    @threads for (data_range, ichunk) in chunks(data, nchunks)\n",
    "        for idata in data_range\n",
    "            old_sum = psums[ichunk]\n",
    "            new_sum = old_sum + data[idata]\n",
    "            psums[ichunk] = new_sum\n",
    "        end\n",
    "    end\n",
    "    return psums\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(partial_sums_safe_chunks(data)) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this chunking scheme also isn't \"thread-biased\" anymore in the sense that we can choose `nchunks != nthreads()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling options\n",
    "\n",
    "Syntax: `@threads [schedule] for ...`\n",
    "\n",
    "  * `:dynamic` (**default**)\n",
    "    * creates O(`nthreads()`) many tasks each processing a contigious region of the iteration space\n",
    "    * each task essentially spawned with `@spawn`\n",
    "      * -> task migration\n",
    "      * -> composability / nestability\n",
    "    \n",
    "  * `:static`\n",
    "    * evenly splits up the iteration space and creates one task per block\n",
    "    * **statically** maps tasks to threads, specifically: task 1 -> thread 1, task 2 -> thread 2, etc.\n",
    "      * -> no task migration, i.e. **fixed task-thread mapping**\n",
    "      * -> not composable / nestable\n",
    "      * -> only little overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task migration (!)**: \n",
    "\n",
    "(Technically, our `tmap` example above is ill-defined.)\n",
    "* **Spawning tasks on specific threads**: Julia doesn't have a built-in tool for this (as of now). However, some packages like [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl) export `@tspawnat <threadid> ...` which allows to spawn *sticky* tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x00000001127753c0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on thread 2\n"
     ]
    }
   ],
   "source": [
    "using ThreadPinning\n",
    "\n",
    "@tspawnat 2 println(\"running on thread \", threadid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:2*nthreads()\n",
    "    println(i, \" -> thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:2*nthreads()\n",
    "    println(i, \" -> thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `@threads :static`, every thread handles precisely two iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:3\n",
    "    @threads :dynamic for j in 1:3\n",
    "        println(\"$i, $j\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:3\n",
    "    @threads :static for j in 1:3\n",
    "        println(\"$i, $j\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_nonuniform_spawn!(a, niter = zeros(Int, nthreads()), load = zeros(Int, nthreads()))\n",
    "    @sync for i in 1:length(a)\n",
    "        Threads.@spawn begin\n",
    "            a[i] = sum(abs2, rand() for j in 1:i)\n",
    "            \n",
    "            # only for bookkeeping\n",
    "            niter[threadid()] += 1\n",
    "            load[threadid()] += i\n",
    "        end\n",
    "    end\n",
    "    return niter, load\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*20)\n",
    "niter, load = compute_nonuniform_spawn!(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "b1 = bar(niter, xlab=\"threadid\", ylab=\"# iterations\", title=\"Number of iterations\", legend=false)\n",
    "b2 = bar(load, xlab=\"threadid\", ylab=\"workload\", title=\"Workload\", legend=false)\n",
    "\n",
    "display(b1)\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_nonuniform_threads!(a, niter = zeros(Int, nthreads()), load = zeros(Int, nthreads()))\n",
    "    @threads for i in 1:length(a)\n",
    "        a[i] = sum(abs2, rand() for j in 1:i)\n",
    "\n",
    "        # only for bookkeeping\n",
    "        niter[threadid()] += 1\n",
    "        load[threadid()] += i\n",
    "    end\n",
    "    return niter, load\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*20)\n",
    "niter, load = compute_nonuniform_threads!(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = bar(niter, xlab=\"threadid\", ylab=\"# iterations\", title=\"Number of iterations\", legend=false)\n",
    "b2 = bar(load, xlab=\"threadid\", ylab=\"workload\", title=\"Workload\", legend=false)\n",
    "\n",
    "display(b1)\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(There might be a scheduling option for `@threads` that implements load-balancing in the future.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading: Things to be aware of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race conditions and thread safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_serial(x)\n",
    "    s = zero(eltype(x))\n",
    "    for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_naive(x)\n",
    "    s = zero(eltype(x))\n",
    "    @threads for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = rand(nthreads()*10_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(numbers);\n",
    "@show sum_serial(numbers);\n",
    "@show sum_threads_naive(numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong** result! Even worse, it's **non-deterministic** and different every time! It's also slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_serial($numbers);\n",
    "@btime sum_threads_naive($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: There is a [race condition](https://en.wikipedia.org/wiki/Race_condition).\n",
    "\n",
    "Note that race conditions aren't specific to reductions. More generally, they can appear when multiple threads are modifying a shared \"global\" state simultaneously.\n",
    "\n",
    "Not all of Julia and its packages in the ecosystem are thread-safe! In general, it is safer to assume that they're not unless proven otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix 1: Divide the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_subsums(x)\n",
    "    blocksize = length(x) ÷ nthreads()\n",
    "    @assert isinteger(blocksize)\n",
    "    idcs = collect(Iterators.partition(1:length(x), blocksize))\n",
    "    \n",
    "    subsums = zeros(eltype(x), nthreads())\n",
    "    @threads for tid in 1:nthreads()\n",
    "        for i in idcs[tid]\n",
    "            @inbounds subsums[tid] += x[i]\n",
    "        end\n",
    "    end\n",
    "    return sum(subsums)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(numbers);\n",
    "@show sum_serial(numbers);\n",
    "@show sum_threads_subsums(numbers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_threads_subsums($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speedup and correct result. But not ideal:\n",
    "\n",
    "* cumbersome to do this manually\n",
    "* can have more subtle performance issues like [false sharing](https://en.wikipedia.org/wiki/False_sharing#:~:text=In%20computer%20science%2C%20false%20sharing,managed%20by%20the%20caching%20mechanism.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix 2: Atomics\n",
    "\n",
    "See [Atomic Operations](https://docs.julialang.org/en/v1/manual/multi-threading/#Atomic-Operations) in the Julia doc for more information. But in generaly one shouldn't avoid using them as much as possible since they actually limit the parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage collection\n",
    "\n",
    "[As of now](https://www.youtube.com/watch?v=Ks0p6PQyIPs), **Julia's GC is not parallel** and doesn't work nicely with multithreading.\n",
    "\n",
    "If it gets triggered, it essentially \"stops the world\" (all threads) for clearing up memory.\n",
    "\n",
    "Hence, when using multithreading, it is even more important to **avoid heap allocations!**\n",
    "\n",
    "(If you can't avoid allocations, consider using multiprocessing instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level tools for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl)\n",
    "\n",
    "*Parallelized Base functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ThreadsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreadsX.sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime ThreadsX.sum($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [FLoops.jl](https://github.com/JuliaFolds/FLoops.jl)\n",
    "\n",
    "*Fast sequential, threaded, and distributed for-loops for Julia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using FLoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_floops(x)\n",
    "    @floop for xi in x\n",
    "        @reduce(s = zero(eltype(x)) + xi)\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_floops($numbers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = rand(nthreads()*10_000);\n",
    "\n",
    "sum_floops(numbers) ≈ sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_serial($numbers);\n",
    "@btime sum_floops($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@floop` supports different *executors* that allow for easy switching between serial and threaded execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_floops(x, executor)\n",
    "    @floop executor for xi in x\n",
    "        @reduce(s += xi)\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_floops($numbers, $(SequentialEx()));\n",
    "@btime sum_floops($numbers, $(ThreadedEx()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more [executors](https://juliafolds.github.io/FLoops.jl/stable/tutorials/parallel/#tutorials-executor), like `DistributedEx` or `CUDAEx`. See, e.g., [FoldsThreads.jl](https://github.com/JuliaFolds/FoldsThreads.jl) and [FoldsCUDA.jl](https://github.com/JuliaFolds/FoldsCUDA.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, FLoops is built on top of [Transducers.jl](https://juliafolds.github.io/Transducers.jl/stable/tutorials/tutorial_parallel/) (i.e. it translates for-loop semantics into folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tullio.jl](https://github.com/mcabbott/Tullio.jl)\n",
    "\n",
    "*Tullio is a very flexible einsum macro* ([Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tullio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(10,10)\n",
    "B = rand(10,10)\n",
    "\n",
    "C = @tullio C[i,j] := A[i,k] * B[k,j] # matrix multiplication\n",
    "\n",
    "C ≈ A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tullio(xs) = @tullio S := xs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_tullio($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Uses `fastmath` and other tricks to be faster here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "\n",
    "*Macro(s) for vectorizing loops.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LoopVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_turbo(x)\n",
    "    s = zero(eltype(x))\n",
    "    @tturbo for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_turbo($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Uses all kinds of SIMD tricks to be faster than the others.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System topology and thread affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hawk compute node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/lstopo_hawk.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not pinning threads (or pinning them badly) can degrade performance massively!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning Julia threads to CPU threads\n",
    "\n",
    "What about external tools like `numactl`, `taskset`, etc.? Doesn't work reliably because it [can't distinguish](https://discourse.julialang.org/t/thread-affinitization-pinning-julia-threads-to-cores/58069/5) between Julia threads and other internal threads.\n",
    "\n",
    "**Options:**\n",
    "\n",
    "* Environment variable: `JULIA_EXCLUSIVE=1` (compact pinning)\n",
    "* More control and convenient visualization: [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl)\n",
    "  * `compact`: pin to cpu thread 0, 1, 2, 3, ... one after another\n",
    "  * `spread`: alternate between sockets so, e.g., 0, 64, 1, 65, 2, 66, .... (if a socket has 64 cores)\n",
    "  * `numa`: same as `spread` but alternate between NUMA domains so, e.g., 0, 16, 32, 48, 64, .... (if a NUMA domain has 16 cores)\n",
    "  * **Caveat:** currently one works on Linux.\n",
    "\n",
    "<img src=\"../imgs/threadinfo.png\" width=1000px>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
