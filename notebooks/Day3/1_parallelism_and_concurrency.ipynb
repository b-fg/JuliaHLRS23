{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel computing is a programming method that **harnesses the power of multiple processors (typically CPU cores) at once**.\n",
    "\n",
    "There are many types of parallelism, some of which are (from micro to macro)\n",
    "\n",
    "* **Instruction level parallelism** (e.g. SIMD)\n",
    "* **Multi-threading** (shared memory)\n",
    "* **Multi-processing** (shared system memory)\n",
    "* **Distributed processing** (typically no shared memory)\n",
    "\n",
    "**Import note before we start: At the center of an efficient parallel code is a fast serial code!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why go parallel?\n",
    "\n",
    "<img src=\"./imgs/42-years-processor-trend.svg\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to go parallel?\n",
    "\n",
    "* If parts of your (optimized!) serial code aren't fast enough.\n",
    "  * note that parallelization typically increases the code complexity\n",
    "* If your system has multiple execution units (CPU threads, GPU threads, ...).\n",
    "  * particularly import on large supercomputers but also already on modern desktop computers and laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many CPU threads / cores do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Hwloc\n",
    "Hwloc.num_physical_cores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there may be more than one CPU thread per physical CPU core (e.g. hyperthreading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.CPU_THREADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many CPU threads / cores does Hawk have?\n",
    "\n",
    "[Hawk has about 720k CPU cores!](https://www.hlrs.de/solutions/systems/hpe-apollo-hawk)\n",
    "\n",
    "Even if you only use a **single node** you have access to 128 CPU cores (64 per CPU). Hence, if you would use only a single core, the node utilization would be less than 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hawk compute node\n",
    "\n",
    "<img src=\"./imgs/lstopo_hawk.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amdahl's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive strong scaling expectation: I have 4 cores, give me my 4x speedup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If $p$ is the fraction of a code that can be parallelized than the maximal theoretical speedup by parallelizing on $n$ cores is given by\n",
    ">$$ F(n) = 1/(1-p + p/n) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "F(p,n) = 1/(1-p + p/n)\n",
    "\n",
    "pl = plot()\n",
    "for p in reverse(sort(vcat(0.2:0.2:1, [0.9, 0.95])))\n",
    "    plot!(pl, n -> F(p,n), 1:16, lab=\"$(Int(p*100))%\", lw=2,\n",
    "        legend=:topleft, xlab=\"number of cores\", ylab=\"parallel speedup\", frame=:box)\n",
    "end\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Parallel computing](https://docs.julialang.org/en/v1/manual/parallel-computing/) in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia provides support for all types of parallelism mentioned above\n",
    "\n",
    "* **Instruction level parallelism** (e.g. SIMD) $\\quad \\quad$ → `@simd`, [SIMD.jl](https://github.com/eschnett/SIMD.jl), [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "* **Multi-threading** (shared memory)           $\\quad \\quad$ → `Base.Threads`, [FLoops.jl](https://github.com/JuliaFolds/FLoops.jl), [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl) ...\n",
    "* **Multi-processing** (shared system memory)   $\\quad \\quad$ → `Distributed` standard library, [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "* **Distributed processing** (typically no shared memory) $\\quad \\quad$ → `Distributed` standard library, [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    "We already talked about SIMD. Let's move on to the next level → **Multithreading**."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
