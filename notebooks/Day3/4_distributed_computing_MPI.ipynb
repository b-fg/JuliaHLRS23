{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing: Message Passing Interface (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and MPI.jl\n",
    "\n",
    "* **[MPI](https://www.mpi-forum.org/)**: A [standard](https://www.mpi-forum.org/docs/) with several specific implementations (e.g. [OpenMPI](https://www.open-mpi.org/), [IntelMPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html#gs.73krlr), [MPICH](https://www.mpich.org/))\n",
    "* **[MPI.jl](https://github.com/JuliaParallel/MPI.jl)**: Julia package and interface to (most) MPI implementations ([paper](https://proceedings.juliacon.org/papers/10.21105/jcon.00068))\n",
    "\n",
    "### Getting MPI\n",
    "\n",
    "By default, an appropriate MPI will be automatically downloaded when adding MPI.jl to a Julia environment (see e.g. MPICH_jll.jl)). Works out of the box almost all of the time.\n",
    "\n",
    "However, in particular on HPC clusters, one sometimes wants/needs to use a **system-wide MPI** installation. Potential reasons include:\n",
    "\n",
    "* Vendor-specific MPI required for MPI to work at all.\n",
    "* Fine-tuned MPI configuration necessary for best performance.\n",
    "* CUDA-aware or ROCm-aware MPI\n",
    "\n",
    "#### How to use a system MPI?\n",
    "\n",
    "```julia\n",
    "using MPIPreferences\n",
    "MPIPreferences.use_system_binary()\n",
    "```\n",
    "If you do this before adding MPI.jl, no MPI will be downloaded.\n",
    "\n",
    "For more, check out the [MPI.jl documentation](https://juliaparallel.org/MPI.jl/stable/configuration/#configure_system_binary).\n",
    "\n",
    "### MPI: Programming model\n",
    "\n",
    "Sinlge Program Multiple Data (SPMD):\n",
    "* **all processes execute the same code** but have different IDs (rank).\n",
    "* **conditionals can be used to get different behavior** for different MPI ranks\n",
    "* individual processes flow at there own pace, **they can (and will) get out of sync**\n",
    "* selecting the concrete number of processes is deferred to \"runtime\"\n",
    "\n",
    "#### Basic example: Hello World\n",
    "\n",
    "```julia\n",
    "# file: mpi_examples/mpi_hello.jl\n",
    "using MPI\n",
    "\n",
    "MPI.Init()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "print(\"Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\\n\")\n",
    "\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental MPI functions\n",
    "\n",
    "* `MPI.Init()` and `MPI.Finalize()`: Always at the top or bottom of your code, respectively.\n",
    "\n",
    "* `MPI.COMM_WORLD`: default *communicator* that includes all processes (MPI ranks)\n",
    "\n",
    "* `MPI.Comm_rank(comm)`: unique rank of the process calling this function\n",
    "\n",
    "* `MPI.Comm_size(comm)`: total number of processes for the given communicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running an MPI code\n",
    "\n",
    "There are `mpirun` and `mpiexec` (or `srun` on SLURM clusters) to run MPI applications. These work fine with Julia.\n",
    "\n",
    "However, MPI.jl provides a [`mpiexecjl` wrapper](https://juliaparallel.org/MPI.jl/stable/configuration/#Julia-wrapper-for-mpiexec) that you can/should use if\n",
    "* you want to use an MPI that has been installed automatically by MPI.jl\n",
    "* want to use different MPIs for different Julia environments\n",
    "\n",
    "**In this course, you should use the following command to run your MPI application (both on the laptops and on Hawk):**\n",
    "\n",
    "`mpiexecjl --project -n N julia mycode.jl`\n",
    "\n",
    "Here, `N` is the desired number of MPI ranks.\n",
    "\n",
    "<img src=\"./imgs/julia_mpi_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI.jl vs MPI in C\n",
    "\n",
    "General rule: `MPI_*` in C → `MPI.*` in Julia, e.g.\n",
    "\n",
    "  * `MPI_COMM_WORLD` → `MPI.COMM_WORLD`\n",
    "  * `MPI_Comm_size` → `MPI.Comm_size`\n",
    "  \n",
    "In principle, you can use the \"low-level\" API under `MPI.API.*`, which is essentially identical to what you might know from C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conveniences of MPI.jl\n",
    "\n",
    "* Julia MPI functions can have **less function arguments** than C counterparts if some of them are deducible/optional.\n",
    "* MPI functions can often handle data of **built-in and custom Julia types** (i.e. custom `struct`s)\n",
    "  * `MPI.Types.create_*` constructor functions (`create_vector`, `create_subarray`, `create_struct`, etc.) get automatically called under the hood.\n",
    "* MPI Functions can often handle **built-in and custom Julia functions**, e.g. as a reducer function in `MPI.Reduce`. (see `mpi_examples/6_mpi_custom.jl`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic MPI functions\n",
    "\n",
    "### Point-to-point communication (**blocking**)\n",
    "\n",
    "* **Sending:** `MPI.Send(buf, comm; dest, tag=0)`\n",
    "  * `buf`: data buffer, typically an array\n",
    "  * `communicator`\n",
    "  * `destination`: target rank (to receive the data)\n",
    "  * `tag` (optional)\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Receiving:** `MPI.Recv!(recvbuf, comm; source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG)`\n",
    "  * `recvbuf`: buffer to store the received data in, typically an array\n",
    "  * `communicator`\n",
    "  * `source`: source rank (whos sending the data)\n",
    "  * `tag` (optional)\n",
    "  \n",
    "Basic example → see `mpi_examples/2_mpi_basic_communication.jl`\n",
    "\n",
    "**Blocking can lead to deadlocks!**\n",
    "\n",
    "<img src=\"./imgs/MPI-deadlock.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-point communication (**non-blocking**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(essentially the same function signatures, just different names and behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sending:** `MPI.Isend(buf, comm; dest, tag=0)`\n",
    "  * `buf`: data buffer, typically an array\n",
    "  * `communicator`\n",
    "  * `destination`: target rank (to receive the data)\n",
    "  * `tag` (optional)\n",
    "  * Returns a request object (which one may use for waiting operations)\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Receiving:** `MPI.Irecv!(recvbuf, comm; source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG)`\n",
    "  * `recvbuf`: buffer to store the received data in, typically an array\n",
    "  * `communicator`\n",
    "  * `source`: source rank (whos sending the data)\n",
    "  * `tag` (optional)\n",
    "  * Returns a request object (which one may use for waiting operations)\n",
    "  \n",
    "Basic example → see `mpi_examples/3_mpi_basic_communication_nonblocking.jl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective communication\n",
    "\n",
    "* **Explicit synchronization**\n",
    "  * `MPI.Barrier(comm)`: stop execution until all ranks have reached the barrier\n",
    "* **Data movement**:\n",
    "    * Example: Broadcasting, see [here](https://juliaparallel.org/MPI.jl/stable/reference/collective/#Broadcast) and exercise `mpi_bcast`\n",
    "* **Collective computation**:\n",
    "    * Example: Reduction, see [here](https://juliaparallel.org/MPI.jl/stable/reference/collective/#Reduce/Scan) and `mpi_examples/5_mpi_reduction.jl` for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "* **Time measurement**\n",
    "    * `MPI.Wtime()`: the difference between two `MPI.Wtime()` calls is the elapsed wall-clock time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level tools\n",
    "\n",
    "* [PartitionedArrays.jl](https://github.com/fverdugo/PartitionedArrays.jl): Data-oriented parallel implementation of partitioned vectors and sparse matrices needed in FD, FV, and FE simulations.\n",
    "* [Elemental.jl](https://github.com/JuliaParallel/Elemental.jl): A package for dense and sparse distributed linear algebra and optimization.\n",
    "* [PETSc.jl](): Suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. ([original website](https://petsc.org/release/))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
