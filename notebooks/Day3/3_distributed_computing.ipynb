{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing and Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**\n",
    "* **single Julia process -> multiple Julia processes** that coordinate to perform certain computations\n",
    "\n",
    "**Why**\n",
    "* **Scaling things up**: run computations on multiple CPU cores, potentially even on different machines, e.g. nodes of a supercomputer or a local cluster of desktop machines.\n",
    "* Effectively increase your memory: process a large dataset, which wouldn't fit into local memory, in parallel across multiple machines with separate dedicated RAM.\n",
    "* (Typically avoids some multithreading / shared memory computing issues: e.g. multiple GCs.)\n",
    "\n",
    "**Julia provides two fundamental implementations and paradigms**\n",
    "* Julia's built-in [Distributed.jl](https://docs.julialang.org/en/v1/stdlib/Distributed/) standard library\n",
    "* [Message Passing Interface (MPI)](https://www.mpi-forum.org/) through [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed.jl (standard library) vs MPI\n",
    "\n",
    "**Distributed.jl**\n",
    "* convenient for **\"ad-hoc\" distributed computing** (e.g. data processing)\n",
    "* intuitive **master-worker model** (often naturally aligns with the structure of scientific computations)\n",
    "* **interactivity**, e.g. in a REPL / in Jupyter\n",
    "* built-in, no external setup necessary\n",
    "* higher overhead than MPI and doesn't scale as well (by default doesn't utilizie Infiniband/OmniPath)\n",
    "\n",
    "**MPI**\n",
    "* **de-facto industry standard** for massively parallel computing, e.g. large scale distributed computing\n",
    "* **known to scale well** up to thousands of compute nodes\n",
    "* Single Program Multiple Data (SPMD) programming model can be more challenging (at least at first)\n",
    "* **No (or very poor) interactivity** (see [MPIClusterManager.jl](https://github.com/JuliaParallel/MPIClusterManagers.jl))\n",
    "\n",
    "\n",
    "The focus of this notebook is on **Distributed.jl** (MPI â†’ later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed.jl (standard library)\n",
    "\n",
    "Julia's Distributed.jl follows a master-worker paradigm for its native distributed parallelism: **One master process coordinates all the worker processes, which perform the actual computations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers() # the master is considered a worker as long as there are no real workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the number of workers, i.e. Julia processes, from within a Julia session we can dynamically call **`addprocs`**.\n",
    "\n",
    "Alternatively, when starting Julia from the command line, one can use the `-p` option up front. Example,\n",
    "\n",
    "```\n",
    "julia -p 4\n",
    "```\n",
    "\n",
    "will start Julia with 5 processes, 1 master and 4 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every process has a Julia internal `pid` (process id). The master is always 1. You can get the workers pids from `workers()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 4 worker's pids aren't necessarily 2, 3, 4 and 5 and one shouldn't rely on those literal values. Let's remove the processes and add them once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmprocs(workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers() # only the master is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One master to rule them all - `@spawn`, `@spawnat`, `@fetch`, `@fetchfrom`, `@everywhere`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute commands and start computations on workers we can use the following macros\n",
    "\n",
    "* `@spawn`: run a command or a code block on any worker and return a `Future` (a wrapped `Task`) to it's result.\n",
    "* `@spawnat`: same as `@spawn` but one can choose a specific worker by providing its pid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the conceptual similarity between `Threads.@spawn` (task â†’ thread) and `Distributed.@spawn` (task â†’ process) and also `@async`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = @spawn 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the combination of spawning at fetching is so common, there is `@fetch` (blocking) which combines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which worker did the work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch begin\n",
    "    println(myid());\n",
    "    3+3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `@spawnat` and `@fetchfrom` we can delegate the work to a specific worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetchfrom 7 begin\n",
    "    println(myid());\n",
    "    3+3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `@sync` as a blocker to wait for all workers to complete their tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync begin\n",
    "    pids = workers()\n",
    "    @spawn (sleep(2); println(\"Today is reverse day!\"))\n",
    "    @spawn (sleep(1); println(\" Stuttgart!\"))\n",
    "    @spawn println(\"Hello\")\n",
    "end;\n",
    "println(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we understood all that, let's delegate a *complicated* calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function complicated_calculation()\n",
    "    sleep(1) # so complex that it takes a long time :)\n",
    "    randexp(5)\n",
    "end\n",
    "\n",
    "@fetch complicated_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Every worker is a separate Julia process.** (Think of having multiple Julia REPLs open at once.)\n",
    "\n",
    "We only defined `complicated_calculation()` on the master process. The function doesn't exist on any of the workers yet.\n",
    "\n",
    "The macro `@everywhere` allows us to perform steps on all processes (master and worker). This is particularly useful for loading packages and functions definitions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin # execute this block on all workers\n",
    "    using Random\n",
    "    \n",
    "    function complicated_calculation()\n",
    "        sleep(1)\n",
    "        randexp(5) # lives in Random\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch complicated_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a crucial difference between the following two pieces of code. Can you guess what it is? (without reading on ðŸ˜‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function method1()\n",
    "    A = rand(100,100)\n",
    "    B = rand(100,100)\n",
    "    C = @fetch A^2 * B^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function method2()\n",
    "    C = @fetch rand(100,100)^2 * rand(100,100)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime method1();\n",
    "@btime method2();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 is slower, because `A` and `B` are created on the master process, transferred to a worker, and squared and multiplied on the worker process before the result is finally transferred back to the master.\n",
    "\n",
    "Method 2, on the other hand, creates, squares, and multiplies the random matrix all on the work process and only submits the result to the master.\n",
    "\n",
    "Hence, `method1` is **transferring 3x as much data** between the master and the worker!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy example, it's rather easy to identify the faster method.\n",
    "\n",
    "In a real program, however, understanding data movement does require more thought and likely some measurement.\n",
    "\n",
    "For example, if the first process needs matrix `A` in a follow-up computation then the first method might be better in this case. Or, if computing `A` is expensive and only the current process has it, then moving it to another process might be unavoidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer latency at a human scale\n",
    "\n",
    "To understand why thinking about data is important it's instructive to look at the time scales involved in data access.\n",
    "\n",
    "<img src=\"./imgs/latency_human_scales.png\" width=900px>\n",
    "\n",
    "(taken from https://www.prowesscorp.com/computer-latency-at-a-human-scale/)\n",
    "\n",
    "**Efficient data movement is crucial for efficient distributed computing!** (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more tools for explicit data movement like `Channels`/`RemoteChannels` and [ParallelDataTransfer.jl](https://github.com/ChrisRackauckas/ParallelDataTransfer.jl/). For the sake of brevity, we will not cover them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoid globals (once more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myglobal = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function whohas(s::String)\n",
    "    @everywhere begin\n",
    "        var = Symbol($s)\n",
    "        if isdefined(Main, var)\n",
    "            println(\"$var exists.\")\n",
    "        else\n",
    "            println(\"Doesn't exist.\")\n",
    "        end\n",
    "    end\n",
    "    nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whohas(\"myglobal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetchfrom 6 myglobal+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whohas(\"myglobal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globals get copied to workers and continue to exist as globals even after the call.\n",
    "\n",
    "This could lead to **memory accumulation** if many globals are used (just as it would in a single Julia session). â†’ avoid them as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level tools: `@distributed` and `pmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen some of the fundamental building blocks for distributed computing with Distributed.jl. However, in practice, one wants to think as little as possible about how to distribute the work and explicitly spawn tasks.\n",
    "\n",
    "Fortunately, many useful parallel computations do not require (much) data movement at all. A common example is a direct Monte Carlo simulation, where multiple processes can handle independent simulation trials simultaneously. (â†’ **montecarlo_pi** exercise)\n",
    "\n",
    "Julia provides **high-level convenience** tools to\n",
    " * distribute `map` $\\quad$ â†’ $\\quad$ [`pmap`](https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.pmap)\n",
    " * distribute loops $\\quad$ â†’ $\\quad$ [`@distributed`](https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.@distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel map: `pmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often, one simply wants to parallize a `map` operation where a function `f` is applied to all elements of a collection. This is a typical instance of **data parallelism**, which covers a vast class of compute-intensive programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(x->x^2, 1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a pattern can be parallelized in Julia via the high-level function `pmap` (\"parallel map\").\n",
    "\n",
    "**Remark:** No (non-local) data structure mutation in the function `f`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Singular values of multiple matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed, BenchmarkTools; rmprocs(workers()); addprocs(4); nworkers() # clean reset :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using LinearAlgebra\n",
    "\n",
    "M = Matrix{Float64}[rand(200,200) for i = 1:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdvals(rand(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this indeed utilized multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap(M) do m\n",
    "    println(myid());\n",
    "    svdvals(m)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime map($svdvals, $M);\n",
    "@btime pmap($svdvals, $M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed loops (`@distributed`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed, BenchmarkTools; rmprocs(workers()); addprocs(4); nworkers() # clean reset :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Reduction\n",
    "\n",
    "Task: Counting heads in a series of coin tosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function count_heads_loop(n)\n",
    "    c = 0\n",
    "    for i = 1:n\n",
    "        c += rand((0,1))\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "N = 200_000_000\n",
    "@btime count_heads_loop($N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these kinds of computations are called **reductions** (with `+` being the **reducer function**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_heads_reduce(n) = mapreduce(i -> rand((0,1)), +, 1:n)\n",
    "@btime count_heads_reduce($N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize this reduction using\n",
    "* `@distributed (reducer function) for ...`.\n",
    "\n",
    "Note that this has **blocking** character and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function count_heads_distributed_loop(n)\n",
    "    c = @distributed (+) for i in 1:n\n",
    "        rand((0,1))\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime count_heads_distributed_loop($N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed version is about **4x faster**, which is all we could hope for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `@distributed` the work is **evenly distributed** between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function count_heads_distributed_verbose(n)\n",
    "    c = @distributed (+) for i in 1:n\n",
    "        x = rand((0,1))\n",
    "        println(x);\n",
    "        x\n",
    "    end\n",
    "    c\n",
    "end\n",
    "\n",
    "count_heads_distributed_verbose(8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A common mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g(n)\n",
    "    a = 0\n",
    "    @distributed (+) for i in 1:n\n",
    "        a += 1\n",
    "    end\n",
    "    a\n",
    "end\n",
    "\n",
    "a = g(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you expect the value of `a` to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Array mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from `@distributed (reducer) ...` there also is a `@distributed for ...` form. The latter is **non-blocking** and returns a `Task`. (You can think of it as a distributed version of `@spawn` for all the iterations.)\n",
    "\n",
    "However, since the loop body will be executed on different processes, one must be careful to operate on **data structures that are available on all processes** (similar to the mistake highlighted above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function square_broken()\n",
    "    A = collect(1:10)\n",
    "    @sync @distributed for i in eachindex(A)\n",
    "        A[i] = A[i]^2\n",
    "    end\n",
    "    return A\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_broken()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually make all processes operate on the same array, one can use a `SharedArray`. For this to work, the **processes need to live on the same host**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using SharedArrays # must be loaded everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = SharedArray(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function square!(X)\n",
    "    for i in eachindex(X)\n",
    "        sleep(0.001) # mimic some computational cost\n",
    "        X[i] = X[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "function square_distributed!(X)\n",
    "    @sync @distributed for i in eachindex(X)\n",
    "        sleep(0.001) # mimic some computational cost\n",
    "        X[i] = X[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "A = rand(10,10)\n",
    "S = SharedArray(A)\n",
    "\n",
    "@btime square!($A);\n",
    "@btime square_distributed!($S);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the functions `square!` and `square_distributed!` could have also been written with `map` and `pmap`, respectively. (data parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `@distributed` vs `pmap`: When to choose which?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's `pmap` is designed for the case where\n",
    "\n",
    "* one wants to apply **a function to a collection**,\n",
    "* one needs **load-balancing** (e.g. workload is non-uniform), and/or\n",
    "* each function call does enough work to amortize the overhead. \n",
    "\n",
    "On the other hand, `@distributed` is good for\n",
    "\n",
    "* **reductions** (can't be written as `map`/`pmap`),\n",
    "* loops where each iteration **takes about the same time** (uniform workload), and/or\n",
    "* loops where the workload of each iteration is (very) small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level array abstractions: [DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl)\n",
    "\n",
    "In a `DArray`, each process has local access to just a chunk of the data, and no two processes share the same chunk. Processes can be on different hosts.\n",
    "\n",
    "Distributed arrays are for example useful if\n",
    "\n",
    "* Expensive calculations should be performed in parallel on parts of the array on different hosts.\n",
    "* The data doesn't fit into the local machines memory (i.e. loading big files in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed, BenchmarkTools; rmprocs(workers());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that all workers use the same Julia environment\n",
    "addprocs(4; exeflags=\"--project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "@everywhere @show Base.active_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using DistributedArrays, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Matrix{Float64}[rand(200,200) for i = 1:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = distribute(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which workers hold parts of D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which parts do they hold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localpart(D) # the master doesn't hold anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which parts do they hold?\n",
    "for p in workers()\n",
    "    println(@fetchfrom p DistributedArrays.localindices(D))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime map($svdvals, $M);\n",
    "@btime map($svdvals, $D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime pmap($svdvals, $M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Actual* distributed computing with Distributed.jl (some comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating workers on other machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have worked with multiple process on the same system, because we simply used `addprocs(::Integer)`. To put worker processes on other machines, e.g. nodes of a cluster, we need to modify the initial `addprocs` call appropriately.\n",
    "\n",
    "In Julia, starting worker processes is handled by [ClusterManagers](https://docs.julialang.org/en/v1/manual/distributed-computing/#ClusterManagers).\n",
    "\n",
    "* The default one is `LocalManager`. It is automatically used when running `addprocs(i::Integer)` and we have implicitly used it already.\n",
    "* Another important one is `SSHManager`. It is automatically used when running `addprocs(hostnames::Array)`, e.g. `addprocs([\"node123\", \"node456\"])`. The only requirement is a **passwordless ssh access** to all specified hosts.\n",
    "* Cluster managers for SLURM, PBS, and others are provided in [ClusterManagers.jl](https://github.com/JuliaParallel/ClusterManagers.jl). For SLURM, this will make `addprocs` use `srun` under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example*\n",
    "\n",
    "```julia\n",
    "using Distributed\n",
    "\n",
    "addprocs([\"node123\", \"node123\"])\n",
    "\n",
    "@everywhere println(gethostname())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also start multiple processes on different machines:\n",
    "```julia\n",
    "addprocs([(\"node123\", 2), (\"node456\", 3)]) # starts 2 workers on node123 and 3 workers on node456\n",
    "\n",
    "# Use :auto to start as many processes as CPU threads are available\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be aware of different paths:**\n",
    "* By default, `addprocs` expects to find the julia executable on the remote machines under the same path as on the host (master).\n",
    "* It will also try to `cd` to the same folder (set the working directory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from `?addprocs`, `addprocs` takes a bunch of keyword arguments, two of which are of particular importance in this regard:\n",
    "\n",
    "* `dir`: working directory for the worker processes\n",
    "* `exename`: path to julia executable (potentially augmented with pre-commands) for the worker processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "rmprocs(workers())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
