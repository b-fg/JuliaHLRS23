# Exercises

### `1_perf_optimization` (Local/Hawk, Jupyter/VSCode/Terminal)

Given examplatory code snippets, your task will be to optimize them (as much as you can/want to) by utilizing parts of what you've learned in the morning.

(Note: I recommend to play around with this exercise but you shouldn't get stuck here ðŸ˜„)

### `2_cache_sizes` (Hawk, VSCode/Terminal)

By means of a Schoenauer triad kernel, you'll try to measure the effect of the memory hierarchy (i.e. caches) of a CPU in a Hawk node on performance.

### `3_dMMM` (Hawk, VSCode/Terminal)

Here, we'll consider a seemingly simple computation: (dense) matrix-matrix multiplication. You will see that different implementations (e.g. naive vs cache blocking) have vastly different performance.

### `4_SIMD_datadep` (Local/Hawk, Jupyter/VSCode)

In this exercise, you'll be given a for loop with a data dependency that isn't readily vectorizable. You'll learn how you can rewrite the loop to facilitate SIMD and improve the performance.

### `5_likwid` (Local, VSCode/Terminal)

In this exercise you will reproduce the "counting flops" demonstration from the kickoff presentation. We will use Jupyter instead of Pluto though.

